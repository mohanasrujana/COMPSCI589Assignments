{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80026b26-745e-4067-aeaa-5e61a49ea8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp312-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.8.0-cp312-none-macosx_11_0_arm64.whl (73.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed sympy-1.14.0 torch-2.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7f73d61-643f-4ba8-bca0-09f893d6e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def approx_fprime(f, x, eps=1e-8):\n",
    "    \n",
    "    if x.dim() != 1:\n",
    "        raise ValueError(\"Gievn x is not 1D\")\n",
    "\n",
    "    K = x.shape[0]\n",
    "    grad = torch.zeros_like(x, dtype=torch.float64)\n",
    "    x = x.clone().detach().to(torch.float64)\n",
    "\n",
    "    f_x = f(x)  \n",
    "\n",
    "    for i in range(K):\n",
    "        x1 = x.clone()\n",
    "        x1[i] += eps                   \n",
    "        f_x1 = f(x1)\n",
    "        grad[i] = (f_x1 - f_x) / eps\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "455cc0cf-c6d8-4130-b165-a9d97d238462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function(x):\n",
    "    return x[1]**2 + torch.exp(x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "804a559b-85bf-4255-a63e-eac635f0da22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test point 1: x  = [0. 0.]\n",
      "  Numerical gradient = [0.99999999 0.        ]\n",
      "  Autodiff gradient  = [1. 0.]\n",
      "  |Difference|       = [6.07747097e-09 0.00000000e+00]\n",
      "\n",
      "Test point 2: x  = [1. 2.]\n",
      "  Numerical gradient = [2.71828187 3.99999998]\n",
      "  Autodiff gradient  = [2.71828183 4.        ]\n",
      "  |Difference|       = [3.78061702e-08 2.43098839e-08]\n",
      "\n",
      "Test point 3: x  = [-2. -1.]\n",
      "  Numerical gradient = [ 0.13533528 -2.00000001]\n",
      "  Autodiff gradient  = [ 0.13533528 -2.        ]\n",
      "  |Difference|       = [6.91768631e-09 1.00495186e-08]\n"
     ]
    }
   ],
   "source": [
    "def compare_gradients():\n",
    "    eps = 1e-8\n",
    "    test_points = [\n",
    "        torch.tensor([0.0, 0.0], dtype=torch.float64),\n",
    "        torch.tensor([1.0, 2.0], dtype=torch.float64),\n",
    "        torch.tensor([-2.0, -1.0], dtype=torch.float64)\n",
    "    ]\n",
    "\n",
    "    for i, x in enumerate(test_points, 1):\n",
    "        x_autograd = x.clone().detach().requires_grad_(True)\n",
    "        f = test_function(x_autograd)\n",
    "        f.backward()\n",
    "        autodiff = x_autograd.grad.detach()\n",
    "        numerical = approx_fprime(test_function, x, eps=eps)\n",
    "        difference = torch.abs(numerical - autodiff)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\nTest point {i}: x  = {x.numpy()}\")\n",
    "        print(f\"  Numerical gradient = {numerical.numpy()}\")\n",
    "        print(f\"  Autodiff gradient  = {autodiff.numpy()}\")\n",
    "        print(f\"  |Difference|       = {difference.numpy()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    compare_gradients()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea6a2ad-1966-4e4f-b26b-59cfa3060ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
